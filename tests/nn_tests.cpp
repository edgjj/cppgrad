// Copyright (c) 2023 Yegor Suslin
// 
// This software is released under the MIT License.
// https://opensource.org/licenses/MIT

#include <cppgrad/cppgrad.hpp>
#include <gtest/gtest.h>

#include <cmath>
#include <iomanip>
#include <iostream>

using namespace cppgrad;

TEST(NNTests, LinearTest)
{
    nn::Linear lin(16, 10, f64);
    nn::optim::SGD optim(lin, 1e-2);

    auto x = Tensor::full({ 1, 16 }, 0.5, f64);
    auto y = Tensor { { 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 } };

    // change weights/bias
    *lin.get_parameters()[0] = Tensor {
        { -0.1800, -0.0363, -0.1088, -0.1979, -0.1374, -0.1596, -0.1123, -0.1338,
            0.0096, -0.2395, -0.1375, 0.1173, 0.0925, 0.1085, 0.0578, -0.1730 },
        { 0.1088, -0.1767, -0.0799, 0.1648, 0.1975, -0.0768, 0.0303, 0.1974,
            0.0415, 0.2072, -0.1987, 0.0341, -0.2358, -0.0560, 0.2269, 0.1449 },
        { 0.0947, -0.1409, -0.2350, 0.1174, -0.0696, -0.1831, -0.1071, -0.1942,
            0.2378, -0.0042, 0.1715, -0.1392, -0.1365, 0.0651, 0.0046, 0.1542 },
        { -0.2128, -0.1332, 0.0936, -0.1671, -0.2467, 0.1819, 0.1289, -0.1158,
            -0.2387, -0.1597, 0.1892, 0.0770, 0.0226, -0.1530, -0.2051, -0.0050 },
        { -0.2338, 0.1777, 0.2122, 0.0268, -0.1958, 0.1508, 0.2280, -0.2230,
            -0.0600, -0.2448, 0.0524, 0.2108, -0.1909, -0.1068, 0.2057, -0.1222 },
        { -0.0404, -0.2055, 0.0792, -0.0366, 0.1389, -0.0089, -0.1573, 0.1691,
            -0.1694, -0.0053, 0.0883, -0.0471, -0.1516, 0.1597, 0.1227, -0.0593 },
        { -0.0199, 0.0557, -0.1338, -0.2223, 0.0144, -0.1877, 0.1474, 0.1295,
            -0.0668, 0.1917, 0.1197, -0.0579, 0.1074, -0.0184, -0.1402, -0.0085 },
        { 0.0475, 0.2467, 0.0805, 0.1559, -0.1155, 0.2404, -0.0617, 0.0574,
            -0.0752, 0.0742, -0.1224, 0.2341, 0.0290, -0.0682, -0.1807, -0.0501 },
        { 0.0279, -0.0593, -0.0133, -0.1275, 0.1122, 0.0057, 0.1679, 0.1131,
            0.1379, -0.1869, -0.2180, -0.1142, 0.0876, 0.1253, -0.0991, -0.0337 },
        { 0.2152, -0.0253, 0.0347, -0.2090, -0.0950, -0.1985, -0.1288, -0.0020,
            0.1934, -0.2151, 0.0600, -0.1647, 0.0312, -0.1102, 0.1216, 0.0940 }
    };

    *lin.get_parameters()[1] = Tensor { -0.1570, 0.0687, -0.2376, 0.2029, -0.0892, -0.1122, -0.2466, 0.1538,
        -0.0615, -0.1069 };

    for (auto& i : lin.get_parameters()) {
        i->set_requires_grad(true);
    }

    auto out = lin(x)[0];
    auto loss = nn::mse_loss(out, y);

    EXPECT_NEAR(loss.item<f64>(), 0.3900, 1e-4);

    EXPECT_NEAR(out(0, 0).item<f64>(), -0.7722, 1e-4);
    EXPECT_NEAR(out(0, 1).item<f64>(), 0.3335, 1e-4);
    EXPECT_NEAR(out(0, 2).item<f64>(), -0.4198, 1e-4);
    EXPECT_NEAR(out(0, 3).item<f64>(), -0.2691, 1e-4);
    EXPECT_NEAR(out(0, 4).item<f64>(), -0.1456, 1e-4);
    EXPECT_NEAR(out(0, 5).item<f64>(), -0.1739, 1e-4);
    EXPECT_NEAR(out(0, 6).item<f64>(), -0.2914, 1e-4);
    EXPECT_NEAR(out(0, 7).item<f64>(), 0.3997, 1e-4);
    EXPECT_NEAR(out(0, 8).item<f64>(), -0.0987, 1e-4);
    EXPECT_NEAR(out(0, 9).item<f64>(), -0.3062, 1e-4);

    // opt 384 steps
    for (int i = 0; i < 384; i++) {
        auto out = lin(x)[0];
        auto loss = nn::mse_loss(out, y);

        optim.zero_grad();
        loss.backward();
        optim.step();
    }

    out = lin(x)[0];
    loss = nn::mse_loss(out, y);

    EXPECT_NEAR(loss.item<f64>(), 0.0001768690, 1e-4);

    EXPECT_NEAR(out(0, 0).item<f64>(), 0.9622606635, 1e-3);
    EXPECT_NEAR(out(0, 1).item<f64>(), 0.0071007940, 1e-3);
    EXPECT_NEAR(out(0, 2).item<f64>(), -0.0089406967, 1e-3);
    EXPECT_NEAR(out(0, 3).item<f64>(), -0.0057293773, 1e-3);
    EXPECT_NEAR(out(0, 4).item<f64>(), -0.0031018443, 1e-3);
    EXPECT_NEAR(out(0, 5).item<f64>(), -0.0037044957, 1e-3);
    EXPECT_NEAR(out(0, 6).item<f64>(), -0.0062064677, 1e-3);
    EXPECT_NEAR(out(0, 7).item<f64>(), 0.0085126683, 1e-3);
    EXPECT_NEAR(out(0, 8).item<f64>(), -0.0021017604, 1e-3);
    EXPECT_NEAR(out(0, 9).item<f64>(), -0.0065193325, 1e-3);
}